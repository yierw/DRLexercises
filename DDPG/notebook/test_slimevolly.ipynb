{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import slimevolleygym\n",
    "\n",
    "from ddpg import DDPGAgent\n",
    "env = gym.make(\"SlimeVolley-v0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_dim, action_dim = 12, 3\n",
    "\n",
    "BUFFER_SIZE = int(1e6) # replay buffer size\n",
    "BATCH_SIZE = 64        # minibatch size\n",
    "GAMMA = 0.99           # discount factor\n",
    "LR_ACTOR = 1e-4        # learning rate of the actor\n",
    "LR_CRITIC = 1e-4       # learning rate of the critic\n",
    "TAU = 0.01             # soft update\n",
    "UPDATE_EVERY = 10     # update network every X samples added to replay buffer\n",
    "\n",
    "agent = DDPGAgent(obs_dim*2, action_dim,\n",
    "                   lr_actor = LR_ACTOR, lr_critic = LR_CRITIC, \n",
    "                    batch_size = BATCH_SIZE, gamma = GAMMA, tau = TAU, \n",
    "                    buffer_size = BUFFER_SIZE, update_every = UPDATE_EVERY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 50\t score     -4\n",
      "Episode 100\t score     -4\n",
      "Episode 150\t score     -4\n",
      "Episode 200\t score     -4\n",
      "Episode 250\t score     -4\n",
      "Episode 300\t score     -4\n",
      "Episode 350\t score     -4\n",
      "Episode 400\t score     -4\n",
      "Episode 450\t score     -4\n",
      "Episode 499\t score     -4"
     ]
    }
   ],
   "source": [
    "NUM_EPISODES = 500\n",
    "PRINT_EVERY = 50\n",
    "TMAX = 1000\n",
    "\n",
    "for e in range(1, NUM_EPISODES):\n",
    "    score = 0\n",
    "    \n",
    "    obs1 = env.reset()\n",
    "    action = env.action_space.sample()\n",
    "    obs2, reward, done, info = env.step(action)\n",
    "    \n",
    "    obs = np.concatenate([obs1, obs2])\n",
    "    \n",
    "    score = reward\n",
    "    for t in range(TMAX):\n",
    "        \n",
    "        action = agent.get_action(obs, 0.5)\n",
    "        obs1, r1, done, info = env.step(action)\n",
    "        if done:\n",
    "            break\n",
    "        obs2, r2, done, info = env.step(action)\n",
    "        if done:\n",
    "            break\n",
    "        \n",
    "        next_obs = np.concatenate([obs1, obs2])\n",
    "        reward = r1+r2\n",
    "\n",
    "        agent.step(obs, action, reward, next_obs, done)\n",
    "        \n",
    "        obs = next_obs\n",
    "        score += reward\n",
    "        \n",
    "\n",
    "    print('\\rEpisode {}\\t score {:6d}'.format(e, score), end = \"\")\n",
    "    if e % PRINT_EVERY == 0:\n",
    "        print('\\rEpisode {}\\t score {:6d}'.format(e, score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = env.reset()\n",
    "next_obs, reward, done, info = env.step(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym.wrappers import Monitor\n",
    "def wrap_env(env):\n",
    "    env = Monitor(env, './video', force=True)\n",
    "    return env\n",
    "\n",
    "env = wrap_env(gym.make(\"SlimeVolley-v0\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-4"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs1 = env.reset()\n",
    "action = env.action_space.sample()\n",
    "obs2, reward, done, info = env.step(action)\n",
    "obs = np.concatenate([obs1, obs2])\n",
    "    \n",
    "score = reward\n",
    "for t in range(1000):\n",
    "    action = agent.get_action(obs, 0.5)\n",
    "    obs1, r1, done, info = env.step(action)\n",
    "    if done:\n",
    "         break\n",
    "    obs2, r2, done, info = env.step(action)\n",
    "    if done:\n",
    "        break\n",
    "        \n",
    "    next_obs = np.concatenate([obs1, obs2])\n",
    "    reward = r1+r2\n",
    "\n",
    "        \n",
    "    obs = next_obs\n",
    "    score += reward\n",
    "\n",
    "    if done:\n",
    "        break\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
